# Avaliador de Perfis LinkedIn 

AplicaÃ§Ã£o de **anÃ¡lise de perfis do LinkedIn com arquitetura desacoplada**, composta por backend em **FastAPI** e frontend em **JavaScript puro**.  
A ferramenta utiliza **InteligÃªncia Artificial (Google Gemini)** para avaliar candidatos com base em dados coletados via **scraping (Selenium)**.

A coleta dos perfis Ã© feita separadamente por um script Python (`busca_candidatos.py`), que gera o arquivo `.csv` usado na anÃ¡lise principal.

---

## Arquitetura do Projeto

O projeto Ã© composto por trÃªs partes principais:

### ğŸ”¹ Backend (API FastAPI)
- **`main.py`** â†’ Servidor principal da API. ExpÃµe endpoints para salvar vagas, iniciar anÃ¡lises e obter resultados.  
- **`core_analise.py`** â†’ NÃºcleo de lÃ³gica da aplicaÃ§Ã£o, responsÃ¡vel por formatar prompts e interagir com o Google Gemini.  
- **`utils.py`** â†’ FunÃ§Ãµes auxiliares para manipulaÃ§Ã£o de arquivos (`vaga.json`, `dados_extraidos.csv`, `resultados.json`).  
- **`config.py`** â†’ Carrega as variÃ¡veis do `.env` e inicializa o modelo de IA.  

### ğŸ”¹ Frontend (Interface do UsuÃ¡rio)
- **`public/index.html`** â†’ Estrutura principal da pÃ¡gina web.  
- **`public/app.js`** â†’ Controla toda a interaÃ§Ã£o do usuÃ¡rio e a comunicaÃ§Ã£o com a API.  
- **`public/style.css`** â†’ Define o design e a aparÃªncia da interface.  

### ğŸ”¹ Coletor de Dados (Script Selenium)
- **`busca_candidatos.py`** â†’ Script Python responsÃ¡vel por coletar informaÃ§Ãµes de perfis do LinkedIn usando um navegador Chrome logado.

---

## Tecnologias Utilizadas

- **Python 3.10+**
- **FastAPI** â€“ CriaÃ§Ã£o do backend e endpoints REST  
- **Uvicorn** â€“ Servidor ASGI para rodar o FastAPI  
- **HTML5 / CSS3 / JavaScript (Vanilla)** â€“ Interface do usuÃ¡rio  
- **Selenium** â€“ AutomaÃ§Ã£o e scraping de dados do LinkedIn  
- **Pandas** â€“ ManipulaÃ§Ã£o de dados (CSV)  
- **Google Generative AI (Gemini)** â€“ AnÃ¡lise inteligente dos perfis  
- **Python-Dotenv** â€“ Carregamento seguro de variÃ¡veis de ambiente  

---

## Como Executar o Projeto

A execuÃ§Ã£o deve seguir **4 etapas** na ordem correta.

---

### Etapa 1 â€” ConfiguraÃ§Ã£o Inicial (Apenas uma vez)

1. **Clone ou baixe o repositÃ³rio**

   Coloque todos os arquivos em uma pasta local (exemplo: `N2`).

2. **Obtenha sua chave da API do Google Gemini**

   - Acesse o [Google AI Studio](https://aistudio.google.com/).  
   - FaÃ§a login com sua conta Google.  
   - Clique em **"Get API key" â†’ "Create API key in new project"**.  
   - Copie a chave gerada.

3. **Crie o arquivo `.env`**

   Na pasta raiz do projeto (`N2`), crie um arquivo chamado **`.env`** com o conteÃºdo:

   ```
   GOOGLE_API_KEY=SUA_CHAVE_COPIADA_AQUI
   ```

   > **NÃ£o use aspas.**  
   > O arquivo `.gitignore` jÃ¡ impede o envio do `.env` ao GitHub.

4. **Crie e ative o ambiente virtual**

   ```powershell
   python -m venv .venv
   .\.venv\Scripts\activate
   ```

5. **Instale as dependÃªncias**

   ```powershell
   pip install -r requirements.txt
   ```

---

### Etapa 2 â€” Coleta de Dados do LinkedIn (ObrigatÃ³ria)

Esta etapa gera o arquivo `dados_extraidos.csv` com as informaÃ§Ãµes dos perfis.

1. **Abra o Chrome em modo debug**

   ```powershell
   & "C:\Program Files\Google\Chrome\Application\chrome.exe" --remote-debugging-port=9222 --user-data-dir="C:\ChromeDebug"
   ```

2. **FaÃ§a login no LinkedIn**

   Acesse **linkedin.com** e entre com sua conta na janela aberta.

3. **Execute o script de coleta**

   No terminal com o ambiente virtual ativo:

   ```powershell
   python busca_candidatos.py
   ```

   O script conectarÃ¡ ao Chrome logado e visitarÃ¡ os perfis listados em `dataset/candidatos.csv`.  
   ApÃ³s o tÃ©rmino, serÃ¡ criado o arquivo **`dados_extraidos.csv`** na raiz do projeto.

---

### Etapa 3 â€” Iniciar o Servidor da API

No mesmo terminal (com o `.venv` ativo), execute:

```powershell
uvicorn main:app --reload
```

O servidor exibirÃ¡ algo como:
```
Uvicorn running on http://127.0.0.1:8000
```

Deixe este terminal aberto â€” ele Ã© o **backend da aplicaÃ§Ã£o**.

---

### Etapa 4 â€” Usar a AplicaÃ§Ã£o Web

Abra o navegador e acesse:  
 [http://127.0.0.1:8000/](http://127.0.0.1:8000/)

#### Fluxo de uso:
1. **Cadastrar Vaga:**  
   Preencha o formulÃ¡rio â€œ1. Cadastrar Vagaâ€ e clique em **Salvar Vaga**.  
   Isso criarÃ¡ o arquivo `vaga.json`.

2. **Iniciar AnÃ¡lise:**  
   Clique em **Iniciar AnÃ¡lise dos Candidatos** para que a IA processe os dados.

3. **Aguardar:**  
   O tempo depende da quantidade de candidatos.

4. **Atualizar Resultados:**  
   Clique em **Atualizar Resultados** para visualizar o **Top 5** e os **Piores 5** candidatos analisados.

---

## CrÃ©ditos

Desenvolvido por **Alexandre Salgado**  
Projeto acadÃªmico â€“ Engenharia de Software â€“ Universidade CatÃ³lica de Santa Catarina

---

## Estrutura de Pastas

```
/N2/
â”‚
â”œâ”€â”€ dataset/               # Dados de entrada para scraping
â”‚   â””â”€â”€ candidatos.csv
â”‚
â”œâ”€â”€ public/                # Frontend (Interface do UsuÃ¡rio)
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ app.js
â”‚   â””â”€â”€ style.css
â”‚
â”œâ”€â”€ .env                   # Chave da API (ignorado pelo Git)
â”œâ”€â”€ .gitignore             # Ignora arquivos sensÃ­veis (.env, .venv/, etc.)
â”œâ”€â”€ busca_candidatos.py    # Script de scraping (LinkedIn)
â”œâ”€â”€ config.py              # ConfiguraÃ§Ã£o do modelo de IA
â”œâ”€â”€ core_analise.py        # LÃ³gica de anÃ¡lise com Gemini
â”œâ”€â”€ main.py                # Servidor FastAPI
â”œâ”€â”€ requirements.txt       # DependÃªncias Python
â”œâ”€â”€ utils.py               # FunÃ§Ãµes auxiliares (salvar/carregar arquivos)
â”‚
â”œâ”€â”€ dados_extraidos.csv    # Gerado pela coleta de dados
â”œâ”€â”€ vaga.json              # Gerado ao salvar vaga
â””â”€â”€ resultados.json        # Gerado apÃ³s a anÃ¡lise da IA
```
